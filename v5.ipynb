{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mne\n",
    "import pywt\n",
    "from torch.utils.data import DataLoader, Dataset , Subset\n",
    "from torch import nn\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "from torchmetrics.functional.classification.accuracy import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training on device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWTDataset(Dataset):\n",
    "    def __init__(self, db_path, sequence_length=4000):\n",
    "        self.db_path = db_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(\"SELECT COUNT(*) FROM wavelet_transforms\")\n",
    "        self.total_samples = self.cursor.fetchone()[0]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return self.total_samples - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        query = (\n",
    "            \"SELECT cwt_data, target FROM wavelet_transforms WHERE id BETWEEN ? AND ?\"\n",
    "        )\n",
    "\n",
    "        self.cursor.execute(query, (idx + 1, idx + self.sequence_length))\n",
    "        rows = self.cursor.fetchall()\n",
    "\n",
    "        cwt_sequence = np.stack([pickle.loads(row[0]) for row in rows])\n",
    "\n",
    "        target = rows[-1][1]\n",
    "\n",
    "        cwt_tensor = torch.tensor(cwt_sequence, dtype=torch.float32)\n",
    "\n",
    "        target_tensor = torch.tensor(target, dtype=torch.int64)\n",
    "        return cwt_tensor, target_tensor\n",
    "\n",
    "    def __del__(self):\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWTSubset(Dataset): \n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.__getitem__(int(self.indices[idx]))\n",
    "        return row\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CWT_EEG(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        lr,\n",
    "        label_smoothing=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hparams.input_size = input_size\n",
    "        self.hparams.sequence_length = sequence_length\n",
    "        self.hparams.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_of_classes = 3\n",
    "        self.val_percent = 0.01\n",
    "        self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = hn[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # custom\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # only for HP\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": float(\"nan\"),\n",
    "                \"hp/train_acc\": float(\"nan\"),\n",
    "                \"hp/val_loss\": float(\"nan\"),\n",
    "                \"hp/val_acc\": float(\"nan\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": self.trainer.callback_metrics[\"hp/train_loss\"],\n",
    "                \"hp/train_acc\": self.trainer.callback_metrics[\"hp/train_acc\"],\n",
    "                \"hp/val_loss\": self.trainer.callback_metrics[\"hp/val_loss\"],\n",
    "                \"hp/val_acc\": self.trainer.callback_metrics[\"hp/val_acc\"],\n",
    "            },\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        return super().on_test_epoch_end()\n",
    "\n",
    "    def generate_validation_indices(\n",
    "        self, data_length, num_of_val_samples, sequence_length\n",
    "    ):\n",
    "        available_indices = set(range(data_length))\n",
    "        val_indices = []\n",
    "        for _ in range(num_of_val_samples):\n",
    "            if len(available_indices) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\"\n",
    "                )\n",
    "            chosen_index = int(np.random.choice(list(available_indices)))\n",
    "            val_indices.append(chosen_index)\n",
    "            indices_to_remove = set(\n",
    "                range(\n",
    "                    max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "                    min(data_length, chosen_index + (2 * sequence_length) + 3),\n",
    "                )\n",
    "            )\n",
    "            available_indices.difference_update(indices_to_remove)\n",
    "\n",
    "        return val_indices\n",
    "\n",
    "\n",
    "    def generate_train_indices(self, data_length, val_i, sequence_length):\n",
    "        min_distance = sequence_length + 1\n",
    "        mask = np.ones(data_length, dtype=bool)\n",
    "        for index in val_i:\n",
    "\n",
    "            start = max(0, index - min_distance)\n",
    "            end = min(data_length, index + min_distance + 1)\n",
    "\n",
    "            mask[start:end] = False\n",
    "\n",
    "        training_indices = list(np.where(mask)[0])\n",
    "        return training_indices\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds = CWTDataset(\"cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.num_val_samples = int(\n",
    "            len(self.ds) / (4 * self.hparams.sequence_length + 6)\n",
    "        )\n",
    "\n",
    "        val_indices = self.generate_validation_indices(\n",
    "            len(self.ds), self.num_val_samples, self.hparams.sequence_length\n",
    "        )\n",
    "        train_indices = self.generate_train_indices(\n",
    "            len(self.ds), val_indices, self.hparams.sequence_length\n",
    "        )\n",
    "        print(\n",
    "            \"percent of val samples\",\n",
    "            len(val_indices) / (len(val_indices) + len(train_indices)),\n",
    "        )\n",
    "        self.train_set = CWTSubset(self.ds, train_indices)\n",
    "        self.val_set = CWTSubset(self.ds, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=14,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, num_workers=14\n",
    "        )\n",
    "\n",
    "    def get_len_train_val(self):\n",
    "        self.setup()\n",
    "        return len(self.train_set), len(self.val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of val samples 0.041629826112584735\n",
      "(39021, 1695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | loss | CrossEntropyLoss | 0     \n",
      "1 | lstm | LSTM             | 7.9 K \n",
      "2 | fc   | Linear           | 12    \n",
      "------------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.032     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of val samples 0.04164516842338026\n",
      "Epoch 1: 100%|██████████| 3546/3546 [00:46<00:00, 75.81it/s, v_num=2, hp/train_loss_step=0.596, hp/train_acc_step=0.727, hp/val_loss_step=0.037, hp/val_acc_step=1.000, hp/val_loss_epoch=0.653, hp/val_acc_epoch=0.709, hp/train_loss_epoch=0.708, hp/train_acc_epoch=0.674]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3546/3546 [00:46<00:00, 75.79it/s, v_num=2, hp/train_loss_step=0.596, hp/train_acc_step=0.727, hp/val_loss_step=0.037, hp/val_acc_step=1.000, hp/val_loss_epoch=0.653, hp/val_acc_epoch=0.709, hp/train_loss_epoch=0.708, hp/train_acc_epoch=0.674]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = CWT_EEG( batch_size= 11 ,sequence_length =10,input_size=640 , num_layers=3,hidden_size=3, lr=lr).to(device)\n",
    "logger = TensorBoardLogger(\"logs\", name=\"CWT_EEG\",default_hp_metric=False)\n",
    "logger.log_hyperparams(model.hparams, {})\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    "\n",
    ")\n",
    "print(model.get_len_train_val())\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/CWT_EEG_2024-04-26_01-49-50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | loss | CrossEntropyLoss | 0     \n",
      "1 | lstm | LSTM             | 458 K \n",
      "2 | fc   | Linear           | 303   \n",
      "------------------------------------------\n",
      "458 K     Trainable params\n",
      "0         Non-trainable params\n",
      "458 K     Total params\n",
      "1.835     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of val samples 0.04163698445061289\n",
      "Epoch 1: 100%|██████████| 3547/3547 [00:57<00:00, 61.77it/s, v_num=0, hp/train_loss_step=0.00385, hp/train_acc_step=1.000, hp/val_loss_step=0.000197, hp/val_acc_step=1.000, hp/val_loss_epoch=0.104, hp/val_acc_epoch=0.961, hp/train_loss_epoch=0.131, hp/train_acc_epoch=0.953]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3547/3547 [00:57<00:00, 61.73it/s, v_num=0, hp/train_loss_step=0.00385, hp/train_acc_step=1.000, hp/val_loss_step=0.000197, hp/val_acc_step=1.000, hp/val_loss_epoch=0.104, hp/val_acc_epoch=0.961, hp/train_loss_epoch=0.131, hp/train_acc_epoch=0.953]\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logger = loggers.TensorBoardLogger(\"logs\", name=f\"CWT_EEG_{current_time}\", default_hp_metric=False)\n",
    "lr = 0.001\n",
    "model = CWT_EEG(batch_size=11, sequence_length=10, input_size=640, num_layers=3, hidden_size=100, lr=lr).to(device)\n",
    "logger.log_hyperparams(model.hparams)\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_cwt_eeg(config):\n",
    "    model = CWT_EEG(config)\n",
    "    trainer = Trainer(\n",
    "        max_epochs=10,\n",
    "        progress_bar_refresh_rate=0,  # Disable progress bar for clearer logs\n",
    "        callbacks=[TuneReportCallback({\"loss\": \"ptl/val_loss\"}, on=\"validation_end\")],\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),\n",
    "    \"sequence_length\": tune.choice([10, 100, 200]),\n",
    "    \"input_size\": 640,  # This is fixed for our dataset\n",
    "    \"hidden_size\": tune.choice([256, 512, 1024]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"label_smoothing\": 0,\n",
    "}\n",
    "# Uruchomienie procesu optymalizacji\n",
    "analysis = tune.run(\n",
    "    train_cwt_eeg,\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},  # Adjust based on your system's resources\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie folderu dla logów\n",
    "writer = SummaryWriter('./lightning_logs/CWT_EEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(11, 10,640).to(device)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Warstwa LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Inicjalizacja stanu ukrytego i stanu komórki\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "\n",
    "        # Przejście przez LSTM\n",
    "        lstm_out, _ = self.lstm(input_seq, (h0, c0))\n",
    "\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "# Parametry modelu\n",
    "input_size = 20  # Wymiary wejściowe (np. cechy)\n",
    "hidden_size = 10  # Wymiary stanu ukrytego LSTM\n",
    "\n",
    "\n",
    "# Tworzenie instancji modelu\n",
    "model = SimpleLSTM(input_size, hidden_size )\n",
    "\n",
    "dummy_input = torch.randn( 1, 10, 20)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()\n",
    "# Wyświetlenie struktury modelu\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_split(data_length, val_percent, sequence_length):\n",
    "    available_lenght = data_length - sequence_length\n",
    "    val_count = int(val_percent * available_lenght)\n",
    "    all_indices = np.arange(available_lenght)\n",
    "    val_indices = np.sort(np.random.choice(all_indices, size=val_count, replace=False))\n",
    "    mask = np.ones(available_lenght, dtype=bool)\n",
    "    for idx in val_indices:\n",
    "        start = max(0, idx - sequence_length + 1)\n",
    "        end = min(available_lenght, idx + sequence_length)\n",
    "        mask[start:end] = False\n",
    "    train_indices = np.where(mask)[0]\n",
    "    # convert evry number to int\n",
    "    train_indices = list(train_indices)\n",
    "    val_indices = list(val_indices)\n",
    "\n",
    "    return train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_tensorboard_scalars(logdir):\n",
    "    # Tworzy akumulator do odczytu danych\n",
    "    ea = event_accumulator.EventAccumulator(logdir,\n",
    "        size_guidance={event_accumulator.SCALARS: 0})  # 0 = bez ograniczeń\n",
    "    ea.Reload()  # Wczytaj wszystkie dane z dysku\n",
    "\n",
    "    # Odczyt danych skalarnych\n",
    "    scalars = {}\n",
    "    for tag in ea.Tags()['scalars']:\n",
    "        events = ea.Scalars(tag)\n",
    "        scalars[tag] = [(e.wall_time, e.step, e.value) for e in events]\n",
    "\n",
    "    return scalars\n",
    "\n",
    "# Ścieżka do katalogu z logami TensorBoard\n",
    "logdir = '/home/daniel/repos/Decoding_of_EEG/lightning_logs/CWT_EEG/version_51/events.out.tfevents.1713204451.pop-os.32833.5'\n",
    "\n",
    "# Wczytanie danych\n",
    "scalars = load_tensorboard_scalars(logdir)\n",
    "\n",
    "# Przykład wyświetlenia danych skalarnych\n",
    "tag = 'hp/train_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'hp/val_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz set możliwych indeksów\n",
    "    available_indices = set(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(list(available_indices))\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index - (2* min_distance))\n",
    "        end = min(data_length, chosen_index + min_distance)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu z zbioru available_indices\n",
    "        for idx in range(start, end + 1):\n",
    "            available_indices.discard(idx)  # discard nie zgłasza błędu, jeśli element nie istnieje\n",
    "\n",
    "        # Wydrukuj postęp\n",
    "        print(f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 10000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance_forward = sequence_length + 1\n",
    "    min_distance_backward = 2 * sequence_length + 1\n",
    "    print(\"min_distance_forward:\", min_distance_forward)\n",
    "    print(\"min_distance_backward:\", min_distance_backward)\n",
    "    # Utwórz listę możliwych indeksów\n",
    "    available_indices = list(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "        array = np.array(available_indices)\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(available_indices)\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index -  min_distance_backward)\n",
    "        end = min(data_length, chosen_index + min_distance_forward)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu\n",
    "        available_indices = [idx for idx in available_indices if idx < start or idx > end]\n",
    "        print(\"len(available_indices):\", len(available_indices))\n",
    "        # Wydrukuj postęp\n",
    "        print(f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 100\n",
    "num_of_val_samples = 100\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    min_distance = sequence_length + 1  # Minimalna odległość pomiędzy indeksami\n",
    "    available_indices = set(range(data_length))  # Tworzymy zbiór dostępnych indeksów\n",
    "\n",
    "    chosen_indices = []\n",
    "    for _ in range(num_of_val_samples):\n",
    "        if len(available_indices) == 0:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "        \n",
    "        chosen_index = np.random.choice(list(available_indices))  # Losujemy z dostępnych indeksów\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Usuwamy indeksy w zakresie `sequence_length` w obie strony od wybranego indeksu\n",
    "        indices_to_remove = set(range(max(0, chosen_index - (2* sequence_length ) - 3 ),\n",
    "                                      min(data_length, chosen_index + (2 * sequence_length) + 3 )))\n",
    "        available_indices.difference_update(indices_to_remove)  # Aktualizujemy zbiór dostępnych indeksów\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 50\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_val_indices(data_length, val_indices,   sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 77991\n",
    "num_of_val_samples = 100\n",
    "sequence_length = 10\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_train_val_indices(data_length, train_indices, val_indices,   sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "    # if train_indices is not empty:\n",
    "    for index in train_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = index\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='blue', alpha=0.1)  # zakres wokół punktu)\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 5\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    train_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "    plot_train_val_indices(data_length, train_indices, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_validation_indices(data_length,training_indices, val_indices, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"Indeksy zbioru treningowego:\", training_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomlist= [1,2,9,10]\n",
    "index = np.random.choice(randomlist)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 30\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    print(\"len(training_indices):\", len(training_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
