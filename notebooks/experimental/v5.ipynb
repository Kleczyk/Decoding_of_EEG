{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:06.982334Z",
     "start_time": "2024-05-12T16:59:06.975177Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mne\n",
    "import pywt\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch import nn\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.functional.classification.accuracy import accuracy"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:07.320449Z",
     "start_time": "2024-05-12T16:59:07.313617Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training on device: {device}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:07.550672Z",
     "start_time": "2024-05-12T16:59:07.536673Z"
    }
   },
   "source": [
    "class CWTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for EEG data after CWT transformation stored in SQLite database.\n",
    "    \n",
    "    Attributes:\n",
    "        db_path: str - path to SQLite database\n",
    "        sequence_length: int - length of the sequence\n",
    "    Methods:\n",
    "        __len__ - returns the number of samples in the dataset minus the sequence length\n",
    "        __getitem__ - returns a sample from the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db_path, sequence_length=4000):\n",
    "        \"\"\"\n",
    "        Constructor for CWTDataset class that initializes the dataset.\n",
    "        Args:\n",
    "            db_path: str - path to SQLite database\n",
    "            sequence_length: int - length of the sequence\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(\"SELECT COUNT(*) FROM wavelet_transforms\")\n",
    "        self.total_samples = self.cursor.fetchone()[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            int - number of samples in the dataset minus the sequence length\n",
    "        \"\"\"\n",
    "        return self.total_samples - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        function that returns as many samples as the sequence length\n",
    "        Args:\n",
    "            idx: int - index of the sample\n",
    "        Returns:\n",
    "            tuple - (cwt_tensor, target_tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        query = (\n",
    "            \"SELECT cwt_data, target FROM wavelet_transforms WHERE id BETWEEN ? AND ?\"\n",
    "        )\n",
    "\n",
    "        self.cursor.execute(query, (idx + 1, idx + self.sequence_length))\n",
    "        rows = self.cursor.fetchall()\n",
    "\n",
    "        cwt_sequence = np.stack([pickle.loads(row[0]) for row in rows])\n",
    "\n",
    "        target = rows[-1][1]\n",
    "\n",
    "        cwt_tensor = torch.tensor(cwt_sequence, dtype=torch.float32)\n",
    "\n",
    "        target_tensor = torch.tensor(target, dtype=torch.int64)\n",
    "        return cwt_tensor, target_tensor\n",
    "\n",
    "    def __del__(self):\n",
    "        self.conn.close()"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:07.935066Z",
     "start_time": "2024-05-12T16:59:07.926376Z"
    }
   },
   "source": [
    "class CWTSubset(Dataset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.__getitem__(int(self.indices[idx]))\n",
    "        return row\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:08.134666Z",
     "start_time": "2024-05-12T16:59:08.097365Z"
    }
   },
   "source": [
    "class CWT_EEG(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size,\n",
    "            sequence_length,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            label_smoothing=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hparams.input_size = input_size\n",
    "        self.hparams.sequence_length = sequence_length\n",
    "        self.hparams.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_of_classes = 3\n",
    "        self.val_percent = 0.01\n",
    "        self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = hn[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # custom\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # only for HP\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": float(\"nan\"),\n",
    "                \"hp/train_acc\": float(\"nan\"),\n",
    "                \"hp/val_loss\": float(\"nan\"),\n",
    "                \"hp/val_acc\": float(\"nan\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": self.trainer.callback_metrics[\"hp/train_loss\"],\n",
    "                \"hp/train_acc\": self.trainer.callback_metrics[\"hp/train_acc\"],\n",
    "                \"hp/val_loss\": self.trainer.callback_metrics[\"hp/val_loss\"],\n",
    "                \"hp/val_acc\": self.trainer.callback_metrics[\"hp/val_acc\"],\n",
    "            },\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        return super().on_train_epoch_end()\n",
    "\n",
    "    def generate_validation_indices(\n",
    "            self, data_length, num_of_val_samples, sequence_length\n",
    "    ):\n",
    "        available_indices = set(range(data_length))\n",
    "        val_indices = []\n",
    "        for _ in range(num_of_val_samples):\n",
    "            if len(available_indices) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\"\n",
    "                )\n",
    "            chosen_index = int(np.random.choice(list(available_indices)))\n",
    "            val_indices.append(chosen_index)\n",
    "            indices_to_remove = set(\n",
    "                range(\n",
    "                    max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "                    min(data_length, chosen_index + (2 * sequence_length) + 3),\n",
    "                )\n",
    "            )\n",
    "            available_indices.difference_update(indices_to_remove)\n",
    "\n",
    "        return val_indices\n",
    "\n",
    "    def generate_train_indices(self, data_length, val_i, sequence_length):\n",
    "        min_distance = sequence_length + 1\n",
    "        mask = np.ones(data_length, dtype=bool)\n",
    "        for index in val_i:\n",
    "            start = max(0, index - min_distance)\n",
    "            end = min(data_length, index + min_distance + 1)\n",
    "\n",
    "            mask[start:end] = False\n",
    "\n",
    "        training_indices = list(np.where(mask)[0])\n",
    "        return training_indices\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds = CWTDataset(\"/home/daniel/repos/Decoding_of_EEG/data/processed/df_train_cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.num_val_samples = int(\n",
    "            len(self.ds) / (4 * self.hparams.sequence_length + 6)\n",
    "        )\n",
    "\n",
    "        val_indices = self.generate_validation_indices(\n",
    "            len(self.ds), self.num_val_samples, self.hparams.sequence_length\n",
    "        )\n",
    "        train_indices = self.generate_train_indices(\n",
    "            len(self.ds), val_indices, self.hparams.sequence_length\n",
    "        )\n",
    "        print(\n",
    "            \"percent of val samples\",\n",
    "            len(val_indices) / (len(val_indices) + len(train_indices)),\n",
    "        )\n",
    "        self.train_set = CWTSubset(self.ds, train_indices)\n",
    "        self.val_set = CWTSubset(self.ds, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=7,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, num_workers=7\n",
    "        )\n",
    "\n",
    "    def get_len_train_val(self):\n",
    "        self.setup()\n",
    "        return len(self.train_set), len(self.val_set)"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:08.160457Z",
     "start_time": "2024-05-12T16:59:08.142858Z"
    }
   },
   "source": [
    "class CWT_EEG_Another_Person_Val(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size,\n",
    "            sequence_length,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            label_smoothing=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hparams.input_size = input_size\n",
    "        self.hparams.sequence_length = sequence_length\n",
    "        self.hparams.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_of_classes = 3\n",
    "        self.val_percent = 0.01\n",
    "        self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        print(x.shape) \n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = hn[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # custom\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # only for HP\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": float(\"nan\"),\n",
    "                \"hp/train_acc\": float(\"nan\"),\n",
    "                \"hp/val_loss\": float(\"nan\"),\n",
    "                \"hp/val_acc\": float(\"nan\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "        \n",
    "        self.log(\"hp/train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = CWTDataset(\"../../data/processed/df_train_cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.val_set = CWTDataset(\"../../data/processed/df_val_cwt_data.db\", self.hparams.sequence_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=14,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, num_workers=14\n",
    "        )\n",
    "\n",
    "    def get_len_train_val(self):\n",
    "        self.setup()\n",
    "        return len(self.train_set), len(self.val_set)"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T16:59:08.228670Z",
     "start_time": "2024-05-12T16:59:08.218206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class CWT_EEG_CrossPersonValidation(CWT_EEG):\n",
    "    def __init__(self, batch_size, sequence_length, input_size, hidden_size, num_layers, lr, label_smoothing=0):\n",
    "        super().__init__(batch_size, sequence_length, input_size, hidden_size, num_layers, lr, label_smoothing)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = CWTDataset(\"../../data/processed/df_train_cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.val_set = CWTDataset(\"../../data/processed/df_val_cwt_data.db\", self.hparams.sequence_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_set, batch_size=self.hparams.batch_size, num_workers=14,\n",
    "                                           shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_set, batch_size=self.hparams.batch_size, num_workers=14)\n"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T17:00:14.152142Z",
     "start_time": "2024-05-12T16:59:55.781872Z"
    }
   },
   "source": [
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = CWT_EEG(batch_size=11, sequence_length=10, input_size=640, num_layers=3, hidden_size=3,\n",
    "                                      lr=lr).to(device)\n",
    "logger = TensorBoardLogger(\"logs\", name=\"CWT_EEG\", default_hp_metric=False)\n",
    "logger.log_hyperparams(model.hparams, {})\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    "\n",
    ")\n",
    "print(model.get_len_train_val())\n",
    "trainer.fit(model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of val samples 0.0416613007083065\n",
      "(59532, 2588)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type             | Params\n",
      "------------------------------------------\n",
      "0 | loss | CrossEntropyLoss | 0     \n",
      "1 | lstm | LSTM             | 7.9 K \n",
      "2 | fc   | Linear           | 12    \n",
      "------------------------------------------\n",
      "7.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.9 K     Total params\n",
      "0.032     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent of val samples 0.0416613007083065\n",
      "Epoch 0:   4%|▍         | 239/5412 [00:03<01:06, 78.10it/s, v_num=1, hp/train_loss_step=1.030, hp/train_acc_step=0.545] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 13\u001B[0m\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      8\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m      9\u001B[0m     logger\u001B[38;5;241m=\u001B[39mlogger\n\u001B[1;32m     10\u001B[0m \n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(model\u001B[38;5;241m.\u001B[39mget_len_train_val())\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1031\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1033\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1034\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m     batch, _, __ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(data_fetcher)\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     batch_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:133\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatches\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:60\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_profiler()\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterator)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    339\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _ITERATOR_RETURN:\n\u001B[1;32m    340\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator)\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n\u001B[1;32m    343\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001B[0m, in \u001B[0;36m_MaxSizeCycle.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n):\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 78\u001B[0m         out[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miterators[i])\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_consumed[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/_utils.py:705\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    701\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    702\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 705\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 1.\nOriginal Traceback (most recent call last):\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/daniel/miniconda3/envs/EEG311/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import datetime\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logger = loggers.TensorBoardLogger(\"logs\", name=f\"CWT_EEG_{current_time}\", default_hp_metric=False)\n",
    "lr = 0.001\n",
    "model = CWT_EEG_Another_Person_Val(batch_size=11, sequence_length=10, input_size=640, num_layers=3, hidden_size=100, lr=lr).to(device)\n",
    "logger.log_hyperparams(model.hparams)\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.hparams"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_cwt_eeg(config):\n",
    "    model = CWT_EEG(config)\n",
    "    trainer = Trainer(\n",
    "        max_epochs=10,\n",
    "        progress_bar_refresh_rate=0,  # Disable progress bar for clearer logs\n",
    "        callbacks=[TuneReportCallback({\"loss\": \"ptl/val_loss\"}, on=\"validation_end\")],\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),\n",
    "    \"sequence_length\": tune.choice([10, 100, 200]),\n",
    "    \"input_size\": 640,  # This is fixed for our dataset\n",
    "    \"hidden_size\": tune.choice([256, 512, 1024]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"label_smoothing\": 0,\n",
    "}\n",
    "# Uruchomienie procesu optymalizacji\n",
    "analysis = tune.run(\n",
    "    train_cwt_eeg,\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},  # Adjust based on your system's resources\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Utworzenie folderu dla logów\n",
    "writer = SummaryWriter('./lightning_logs/CWT_EEG')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dummy_input = torch.randn(11, 10, 640).to(device)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Warstwa LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Inicjalizacja stanu ukrytego i stanu komórki\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "\n",
    "        # Przejście przez LSTM\n",
    "        lstm_out, _ = self.lstm(input_seq, (h0, c0))\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "\n",
    "# Parametry modelu\n",
    "input_size = 20  # Wymiary wejściowe (np. cechy)\n",
    "hidden_size = 10  # Wymiary stanu ukrytego LSTM\n",
    "\n",
    "# Tworzenie instancji modelu\n",
    "model = SimpleLSTM(input_size, hidden_size)\n",
    "\n",
    "dummy_input = torch.randn(1, 10, 20)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()\n",
    "# Wyświetlenie struktury modelu\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_train_val_split(data_length, val_percent, sequence_length):\n",
    "    available_lenght = data_length - sequence_length\n",
    "    val_count = int(val_percent * available_lenght)\n",
    "    all_indices = np.arange(available_lenght)\n",
    "    val_indices = np.sort(np.random.choice(all_indices, size=val_count, replace=False))\n",
    "    mask = np.ones(available_lenght, dtype=bool)\n",
    "    for idx in val_indices:\n",
    "        start = max(0, idx - sequence_length + 1)\n",
    "        end = min(available_lenght, idx + sequence_length)\n",
    "        mask[start:end] = False\n",
    "    train_indices = np.where(mask)[0]\n",
    "    # convert evry number to int\n",
    "    train_indices = list(train_indices)\n",
    "    val_indices = list(val_indices)\n",
    "\n",
    "    return train_indices, val_indices"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_tensorboard_scalars(logdir):\n",
    "    # Tworzy akumulator do odczytu danych\n",
    "    ea = event_accumulator.EventAccumulator(logdir,\n",
    "                                            size_guidance={event_accumulator.SCALARS: 0})  # 0 = bez ograniczeń\n",
    "    ea.Reload()  # Wczytaj wszystkie dane z dysku\n",
    "\n",
    "    # Odczyt danych skalarnych\n",
    "    scalars = {}\n",
    "    for tag in ea.Tags()['scalars']:\n",
    "        events = ea.Scalars(tag)\n",
    "        scalars[tag] = [(e.wall_time, e.step, e.value) for e in events]\n",
    "\n",
    "    return scalars\n",
    "\n",
    "\n",
    "# Ścieżka do katalogu z logami TensorBoard\n",
    "logdir = '/home/daniel/repos/Decoding_of_EEG/lightning_logs/CWT_EEG/version_51/events.out.tfevents.1713204451.pop-os.32833.5'\n",
    "\n",
    "# Wczytanie danych\n",
    "scalars = load_tensorboard_scalars(logdir)\n",
    "\n",
    "# Przykład wyświetlenia danych skalarnych\n",
    "tag = 'hp/train_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tag = 'hp/val_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz set możliwych indeksów\n",
    "    available_indices = set(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(list(available_indices))\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index - (2 * min_distance))\n",
    "        end = min(data_length, chosen_index + min_distance)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu z zbioru available_indices\n",
    "        for idx in range(start, end + 1):\n",
    "            available_indices.discard(idx)  # discard nie zgłasza błędu, jeśli element nie istnieje\n",
    "\n",
    "        # Wydrukuj postęp\n",
    "        print(\n",
    "            f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 10000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance_forward = sequence_length + 1\n",
    "    min_distance_backward = 2 * sequence_length + 1\n",
    "    print(\"min_distance_forward:\", min_distance_forward)\n",
    "    print(\"min_distance_backward:\", min_distance_backward)\n",
    "    # Utwórz listę możliwych indeksów\n",
    "    available_indices = list(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "        array = np.array(available_indices)\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(available_indices)\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index - min_distance_backward)\n",
    "        end = min(data_length, chosen_index + min_distance_forward)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu\n",
    "        available_indices = [idx for idx in available_indices if idx < start or idx > end]\n",
    "        print(\"len(available_indices):\", len(available_indices))\n",
    "        # Wydrukuj postęp\n",
    "        print(\n",
    "            f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 100\n",
    "num_of_val_samples = 100\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    min_distance = sequence_length + 1  # Minimalna odległość pomiędzy indeksami\n",
    "    available_indices = set(range(data_length))  # Tworzymy zbiór dostępnych indeksów\n",
    "\n",
    "    chosen_indices = []\n",
    "    for _ in range(num_of_val_samples):\n",
    "        if len(available_indices) == 0:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "\n",
    "        chosen_index = np.random.choice(list(available_indices))  # Losujemy z dostępnych indeksów\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Usuwamy indeksy w zakresie `sequence_length` w obie strony od wybranego indeksu\n",
    "        indices_to_remove = set(range(max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "                                      min(data_length, chosen_index + (2 * sequence_length) + 3)))\n",
    "        available_indices.difference_update(indices_to_remove)  # Aktualizujemy zbiór dostępnych indeksów\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 50\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_val_indices(data_length, val_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_train_val_indices(data_length, train_indices, val_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "    # if train_indices is not empty:\n",
    "    for index in train_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = index\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='blue', alpha=0.1)  # zakres wokół punktu)\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 5\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    train_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "    plot_train_val_indices(data_length, train_indices, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_validation_indices(data_length, training_indices, val_indices, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"Indeksy zbioru treningowego:\", training_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "randomlist = [1, 2, 9, 10]\n",
    "index = np.random.choice(randomlist)\n",
    "index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 30\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    print(\"len(training_indices):\", len(training_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install sidekit\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EEG311)",
   "language": "python",
   "name": "eeg311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
