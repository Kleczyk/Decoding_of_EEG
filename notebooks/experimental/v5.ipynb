{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.699666Z",
     "start_time": "2024-05-12T12:24:38.585408Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mne\n",
    "import pywt\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch import nn\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.functional.classification.accuracy import accuracy"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:24:43.749695: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-12 14:24:44.316954: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-12 14:24:45.752765: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.727269Z",
     "start_time": "2024-05-12T12:24:46.701775Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Training on device: {device}')ggi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.741806Z",
     "start_time": "2024-05-12T12:24:46.730062Z"
    }
   },
   "source": [
    "class CWTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for EEG data after CWT transformation stored in SQLite database.\n",
    "    \n",
    "    Attributes:\n",
    "        db_path: str - path to SQLite database\n",
    "        sequence_length: int - length of the sequence\n",
    "    Methods:\n",
    "        __len__ - returns the number of samples in the dataset minus the sequence length\n",
    "        __getitem__ - returns a sample from the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, db_path, sequence_length=4000):\n",
    "        \"\"\"\n",
    "        Constructor for CWTDataset class that initializes the dataset.\n",
    "        Args:\n",
    "            db_path: str - path to SQLite database\n",
    "            sequence_length: int - length of the sequence\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.sequence_length = sequence_length\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(\"SELECT COUNT(*) FROM wavelet_transforms\")\n",
    "        self.total_samples = self.cursor.fetchone()[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Args:\n",
    "            None\n",
    "        Returns:\n",
    "            int - number of samples in the dataset minus the sequence length\n",
    "        \"\"\"\n",
    "        return self.total_samples - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        function that returns as many samples as the sequence length\n",
    "        Args:\n",
    "            idx: int - index of the sample\n",
    "        Returns:\n",
    "            tuple - (cwt_tensor, target_tensor)\n",
    "        \n",
    "        \"\"\"\n",
    "        query = (\n",
    "            \"SELECT cwt_data, target FROM wavelet_transforms WHERE id BETWEEN ? AND ?\"\n",
    "        )\n",
    "\n",
    "        self.cursor.execute(query, (idx + 1, idx + self.sequence_length))\n",
    "        rows = self.cursor.fetchall()\n",
    "\n",
    "        cwt_sequence = np.stack([pickle.loads(row[0]) for row in rows])\n",
    "\n",
    "        target = rows[-1][1]\n",
    "\n",
    "        cwt_tensor = torch.tensor(cwt_sequence, dtype=torch.float32)\n",
    "\n",
    "        target_tensor = torch.tensor(target, dtype=torch.int64)\n",
    "        return cwt_tensor, target_tensor\n",
    "\n",
    "    def __del__(self):\n",
    "        self.conn.close()"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.784269Z",
     "start_time": "2024-05-12T12:24:46.744439Z"
    }
   },
   "source": [
    "class CWTSubset(Dataset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset.__getitem__(int(self.indices[idx]))\n",
    "        return row\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.820506Z",
     "start_time": "2024-05-12T12:24:46.790263Z"
    }
   },
   "source": [
    "class CWT_EEG(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size,\n",
    "            sequence_length,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            label_smoothing=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hparams.input_size = input_size\n",
    "        self.hparams.sequence_length = sequence_length\n",
    "        self.hparams.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_of_classes = 3\n",
    "        self.val_percent = 0.01\n",
    "        self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = hn[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # custom\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # only for HP\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": float(\"nan\"),\n",
    "                \"hp/train_acc\": float(\"nan\"),\n",
    "                \"hp/val_loss\": float(\"nan\"),\n",
    "                \"hp/val_acc\": float(\"nan\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": self.trainer.callback_metrics[\"hp/train_loss\"],\n",
    "                \"hp/train_acc\": self.trainer.callback_metrics[\"hp/train_acc\"],\n",
    "                \"hp/val_loss\": self.trainer.callback_metrics[\"hp/val_loss\"],\n",
    "                \"hp/val_acc\": self.trainer.callback_metrics[\"hp/val_acc\"],\n",
    "            },\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "        return super().on_train_epoch_end()\n",
    "\n",
    "    def generate_validation_indices(\n",
    "            self, data_length, num_of_val_samples, sequence_length\n",
    "    ):\n",
    "        available_indices = set(range(data_length))\n",
    "        val_indices = []\n",
    "        for _ in range(num_of_val_samples):\n",
    "            if len(available_indices) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\"\n",
    "                )\n",
    "            chosen_index = int(np.random.choice(list(available_indices)))\n",
    "            val_indices.append(chosen_index)\n",
    "            indices_to_remove = set(\n",
    "                range(\n",
    "                    max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "                    min(data_length, chosen_index + (2 * sequence_length) + 3),\n",
    "                )\n",
    "            )\n",
    "            available_indices.difference_update(indices_to_remove)\n",
    "\n",
    "        return val_indices\n",
    "\n",
    "    def generate_train_indices(self, data_length, val_i, sequence_length):\n",
    "        min_distance = sequence_length + 1\n",
    "        mask = np.ones(data_length, dtype=bool)\n",
    "        for index in val_i:\n",
    "            start = max(0, index - min_distance)\n",
    "            end = min(data_length, index + min_distance + 1)\n",
    "\n",
    "            mask[start:end] = False\n",
    "\n",
    "        training_indices = list(np.where(mask)[0])\n",
    "        return training_indices\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.ds = CWTDataset(\"cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.num_val_samples = int(\n",
    "            len(self.ds) / (4 * self.hparams.sequence_length + 6)\n",
    "        )\n",
    "\n",
    "        val_indices = self.generate_validation_indices(\n",
    "            len(self.ds), self.num_val_samples, self.hparams.sequence_length\n",
    "        )\n",
    "        train_indices = self.generate_train_indices(\n",
    "            len(self.ds), val_indices, self.hparams.sequence_length\n",
    "        )\n",
    "        print(\n",
    "            \"percent of val samples\",\n",
    "            len(val_indices) / (len(val_indices) + len(train_indices)),\n",
    "        )\n",
    "        self.train_set = CWTSubset(self.ds, train_indices)\n",
    "        self.val_set = CWTSubset(self.ds, val_indices)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=14,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, num_workers=14\n",
    "        )\n",
    "\n",
    "    def get_len_train_val(self):\n",
    "        self.setup()\n",
    "        return len(self.train_set), len(self.val_set)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:46.844297Z",
     "start_time": "2024-05-12T12:24:46.824627Z"
    }
   },
   "source": [
    "class CWT_EEG_another_val_set(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size,\n",
    "            sequence_length,\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            lr,\n",
    "            label_smoothing=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams.batch_size = batch_size\n",
    "        self.hparams.input_size = input_size\n",
    "        self.hparams.sequence_length = sequence_length\n",
    "        self.hparams.lr = lr\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_of_classes = 3\n",
    "        self.val_percent = 0.01\n",
    "        self.loss = torch.nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.num_of_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = hn[-1, :, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # custom\n",
    "    def count_parameters(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # only for HP\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(\n",
    "            self.hparams,\n",
    "            {\n",
    "                \"hp/train_loss\": float(\"nan\"),\n",
    "                \"hp/train_acc\": float(\"nan\"),\n",
    "                \"hp/val_loss\": float(\"nan\"),\n",
    "                \"hp/val_acc\": float(\"nan\"),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y, task=\"multiclass\", num_classes=self.num_of_classes)\n",
    "\n",
    "        self.log(\"hp/val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"hp/val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # def on_train_epoch_end(self) -> None:\n",
    "    #     self.logger.log_hyperparams(\n",
    "    #         self.hparams,\n",
    "    #         {\n",
    "    #             \"hp/train_loss\": self.trainer.callback_metrics[\"hp/train_loss\"],\n",
    "    #             \"hp/train_acc\": self.trainer.callback_metrics[\"hp/train_acc\"],\n",
    "    #             \"hp/val_loss\": self.trainer.callback_metrics[\"hp/val_loss\"],\n",
    "    #             \"hp/val_acc\": self.trainer.callback_metrics[\"hp/val_acc\"],\n",
    "    #         },\n",
    "    #     )\n",
    "    #     self.save_hyperparameters()\n",
    "    #     return super().on_train_epoch_end()\n",
    "\n",
    "    # def generate_validation_indices(\n",
    "    #     self, data_length, num_of_val_samples, sequence_length\n",
    "    # ):\n",
    "    #     available_indices = set(range(data_length))\n",
    "    #     val_indices = []\n",
    "    #     for _ in range(num_of_val_samples):\n",
    "    #         if len(available_indices) == 0:\n",
    "    #             raise ValueError(\n",
    "    #                 \"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\"\n",
    "    #             )\n",
    "    #         chosen_index = int(np.random.choice(list(available_indices)))\n",
    "    #         val_indices.append(chosen_index)\n",
    "    #         indices_to_remove = set(\n",
    "    #             range(\n",
    "    #                 max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "    #                 min(data_length, chosen_index + (2 * sequence_length) + 3),\n",
    "    #             )\n",
    "    #         )\n",
    "    #         available_indices.difference_update(indices_to_remove)\n",
    "\n",
    "    #     return val_indices\n",
    "\n",
    "    # def generate_train_indices(self, data_length, val_i, sequence_length):\n",
    "    #     min_distance = sequence_length + 1\n",
    "    #     mask = np.ones(data_length, dtype=bool)\n",
    "    #     for index in val_i:\n",
    "\n",
    "    #         start = max(0, index - min_distance)\n",
    "    #     end = min(data_length, index + min_distance + 1)\n",
    "\n",
    "    #     mask[start:end] = False\n",
    "\n",
    "    # training_indices = list(np.where(mask)[0])\n",
    "    # return training_indices\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_set = CWTDataset(\"df_train_cwt_data.db\", self.hparams.sequence_length)\n",
    "        self.val_set = CWTDataset(\"df_val_cwt_data.db\", self.hparams.sequence_length)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=14,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, batch_size=self.hparams.batch_size, num_workers=14\n",
    "        )\n",
    "\n",
    "    def get_len_train_val(self):\n",
    "        self.setup()\n",
    "        return len(self.train_set), len(self.val_set)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-12T12:24:47.602098Z",
     "start_time": "2024-05-12T12:24:46.846860Z"
    }
   },
   "source": [
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = CWT_EEG_another_val_set(batch_size=11, sequence_length=10, input_size=640, num_layers=3, hidden_size=3,\n",
    "                                lr=lr).to(device)\n",
    "logger = TensorBoardLogger(\"logs\", name=\"CWT_EEG\", default_hp_metric=False)\n",
    "logger.log_hyperparams(model.hparams, {})\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    "\n",
    ")\n",
    "print(model.get_len_train_val())\n",
    "trainer.fit(model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such table: wavelet_transforms",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 12\u001B[0m\n\u001B[1;32m      6\u001B[0m logger\u001B[38;5;241m.\u001B[39mlog_hyperparams(model\u001B[38;5;241m.\u001B[39mhparams, {})\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      8\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m      9\u001B[0m     logger\u001B[38;5;241m=\u001B[39mlogger\n\u001B[1;32m     10\u001B[0m \n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_len_train_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     13\u001B[0m trainer\u001B[38;5;241m.\u001B[39mfit(model)\n",
      "Cell \u001B[0;32mIn[6], line 145\u001B[0m, in \u001B[0;36mCWT_EEG_another_val_set.get_len_train_val\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_len_train_val\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msetup\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_set), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_set)\n",
      "Cell \u001B[0;32mIn[6], line 128\u001B[0m, in \u001B[0;36mCWT_EEG_another_val_set.setup\u001B[0;34m(self, stage)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msetup\u001B[39m(\u001B[38;5;28mself\u001B[39m, stage\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_set \u001B[38;5;241m=\u001B[39m \u001B[43mCWTDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdf_train_cwt_data.db\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_set \u001B[38;5;241m=\u001B[39m CWTDataset(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdf_val_cwt_data.db\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhparams\u001B[38;5;241m.\u001B[39msequence_length)\n",
      "Cell \u001B[0;32mIn[3], line 26\u001B[0m, in \u001B[0;36mCWTDataset.__init__\u001B[0;34m(self, db_path, sequence_length)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconn \u001B[38;5;241m=\u001B[39m sqlite3\u001B[38;5;241m.\u001B[39mconnect(db_path)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconn\u001B[38;5;241m.\u001B[39mcursor()\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcursor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mSELECT COUNT(*) FROM wavelet_transforms\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_samples \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcursor\u001B[38;5;241m.\u001B[39mfetchone()[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mOperationalError\u001B[0m: no such table: wavelet_transforms"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import datetime\n",
    "from pytorch_lightning import Trainer, loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logger = loggers.TensorBoardLogger(\"logs\", name=f\"CWT_EEG_{current_time}\", default_hp_metric=False)\n",
    "lr = 0.001\n",
    "model = CWT_EEG(batch_size=11, sequence_length=10, input_size=640, num_layers=3, hidden_size=100, lr=lr).to(device)\n",
    "logger.log_hyperparams(model.hparams)\n",
    "trainer = Trainer(\n",
    "    max_epochs=2,\n",
    "    logger=logger\n",
    ")\n",
    "trainer.fit(model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.hparams"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "from ray import tune\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from pytorch_lightning import Trainer, LightningModule\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_cwt_eeg(config):\n",
    "    model = CWT_EEG(config)\n",
    "    trainer = Trainer(\n",
    "        max_epochs=10,\n",
    "        progress_bar_refresh_rate=0,  # Disable progress bar for clearer logs\n",
    "        callbacks=[TuneReportCallback({\"loss\": \"ptl/val_loss\"}, on=\"validation_end\")],\n",
    "    )\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"batch_size\": tune.choice([8, 16, 32, 64]),\n",
    "    \"sequence_length\": tune.choice([10, 100, 200]),\n",
    "    \"input_size\": 640,  # This is fixed for our dataset\n",
    "    \"hidden_size\": tune.choice([256, 512, 1024]),\n",
    "    \"num_layers\": tune.choice([1, 2, 3]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "    \"label_smoothing\": 0,\n",
    "}\n",
    "# Uruchomienie procesu optymalizacji\n",
    "analysis = tune.run(\n",
    "    train_cwt_eeg,\n",
    "    config=search_space,\n",
    "    num_samples=10,\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 1},  # Adjust based on your system's resources\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Utworzenie folderu dla logów\n",
    "writer = SummaryWriter('./lightning_logs/CWT_EEG')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dummy_input = torch.randn(11, 10, 640).to(device)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Warstwa LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # Inicjalizacja stanu ukrytego i stanu komórki\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        h0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "        c0 = torch.zeros(1, input_seq.size(1), self.hidden_size)\n",
    "\n",
    "        # Przejście przez LSTM\n",
    "        lstm_out, _ = self.lstm(input_seq, (h0, c0))\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "\n",
    "# Parametry modelu\n",
    "input_size = 20  # Wymiary wejściowe (np. cechy)\n",
    "hidden_size = 10  # Wymiary stanu ukrytego LSTM\n",
    "\n",
    "# Tworzenie instancji modelu\n",
    "model = SimpleLSTM(input_size, hidden_size)\n",
    "\n",
    "dummy_input = torch.randn(1, 10, 20)  # Przykładowe dane wejściowe (batch_size, input_size)\n",
    "writer.add_graph(model, dummy_input)\n",
    "writer.close()\n",
    "# Wyświetlenie struktury modelu\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def create_train_val_split(data_length, val_percent, sequence_length):\n",
    "    available_lenght = data_length - sequence_length\n",
    "    val_count = int(val_percent * available_lenght)\n",
    "    all_indices = np.arange(available_lenght)\n",
    "    val_indices = np.sort(np.random.choice(all_indices, size=val_count, replace=False))\n",
    "    mask = np.ones(available_lenght, dtype=bool)\n",
    "    for idx in val_indices:\n",
    "        start = max(0, idx - sequence_length + 1)\n",
    "        end = min(available_lenght, idx + sequence_length)\n",
    "        mask[start:end] = False\n",
    "    train_indices = np.where(mask)[0]\n",
    "    # convert evry number to int\n",
    "    train_indices = list(train_indices)\n",
    "    val_indices = list(val_indices)\n",
    "\n",
    "    return train_indices, val_indices"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_tensorboard_scalars(logdir):\n",
    "    # Tworzy akumulator do odczytu danych\n",
    "    ea = event_accumulator.EventAccumulator(logdir,\n",
    "                                            size_guidance={event_accumulator.SCALARS: 0})  # 0 = bez ograniczeń\n",
    "    ea.Reload()  # Wczytaj wszystkie dane z dysku\n",
    "\n",
    "    # Odczyt danych skalarnych\n",
    "    scalars = {}\n",
    "    for tag in ea.Tags()['scalars']:\n",
    "        events = ea.Scalars(tag)\n",
    "        scalars[tag] = [(e.wall_time, e.step, e.value) for e in events]\n",
    "\n",
    "    return scalars\n",
    "\n",
    "\n",
    "# Ścieżka do katalogu z logami TensorBoard\n",
    "logdir = '/home/daniel/repos/Decoding_of_EEG/lightning_logs/CWT_EEG/version_51/events.out.tfevents.1713204451.pop-os.32833.5'\n",
    "\n",
    "# Wczytanie danych\n",
    "scalars = load_tensorboard_scalars(logdir)\n",
    "\n",
    "# Przykład wyświetlenia danych skalarnych\n",
    "tag = 'hp/train_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "tag = 'hp/val_acc_step'  # Zmień na odpowiedni tag, który chcesz wyświetlić\n",
    "times, steps, values = zip(*scalars[tag])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, values, label=tag)\n",
    "plt.xlabel('Krok')\n",
    "plt.ylabel('Wartość')\n",
    "plt.title('Wykres danych z TensorBoard')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz set możliwych indeksów\n",
    "    available_indices = set(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(list(available_indices))\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index - (2 * min_distance))\n",
    "        end = min(data_length, chosen_index + min_distance)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu z zbioru available_indices\n",
    "        for idx in range(start, end + 1):\n",
    "            available_indices.discard(idx)  # discard nie zgłasza błędu, jeśli element nie istnieje\n",
    "\n",
    "        # Wydrukuj postęp\n",
    "        print(\n",
    "            f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 10000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance_forward = sequence_length + 1\n",
    "    min_distance_backward = 2 * sequence_length + 1\n",
    "    print(\"min_distance_forward:\", min_distance_forward)\n",
    "    print(\"min_distance_backward:\", min_distance_backward)\n",
    "    # Utwórz listę możliwych indeksów\n",
    "    available_indices = list(range(data_length))\n",
    "\n",
    "    chosen_indices = []\n",
    "    for i in range(num_of_val_samples):\n",
    "        if not available_indices:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "        array = np.array(available_indices)\n",
    "        # Losuj indeks z dostępnych indeksów\n",
    "        chosen_index = np.random.choice(available_indices)\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Oblicz zakres indeksów do usunięcia\n",
    "        start = max(0, chosen_index - min_distance_backward)\n",
    "        end = min(data_length, chosen_index + min_distance_forward)\n",
    "\n",
    "        # Usuń indeksy zbyt blisko wybranego indeksu\n",
    "        available_indices = [idx for idx in available_indices if idx < start or idx > end]\n",
    "        print(\"len(available_indices):\", len(available_indices))\n",
    "        # Wydrukuj postęp\n",
    "        print(\n",
    "            f\"Postęp: {i + 1}/{num_of_val_samples} indeksów wygenerowanych. ({(i + 1) / num_of_val_samples * 100:.2f}%)\")\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 100\n",
    "num_of_val_samples = 100\n",
    "sequence_length = 200\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"len(val_indices):\", len(val_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_validation_indices(data_length, num_of_val_samples, sequence_length):\n",
    "    min_distance = sequence_length + 1  # Minimalna odległość pomiędzy indeksami\n",
    "    available_indices = set(range(data_length))  # Tworzymy zbiór dostępnych indeksów\n",
    "\n",
    "    chosen_indices = []\n",
    "    for _ in range(num_of_val_samples):\n",
    "        if len(available_indices) == 0:\n",
    "            raise ValueError(\"Nie można wygenerować więcej próbek z uwzględnieniem minimalnego dystansu\")\n",
    "\n",
    "        chosen_index = np.random.choice(list(available_indices))  # Losujemy z dostępnych indeksów\n",
    "        chosen_indices.append(chosen_index)\n",
    "\n",
    "        # Usuwamy indeksy w zakresie `sequence_length` w obie strony od wybranego indeksu\n",
    "        indices_to_remove = set(range(max(0, chosen_index - (2 * sequence_length) - 3),\n",
    "                                      min(data_length, chosen_index + (2 * sequence_length) + 3)))\n",
    "        available_indices.difference_update(indices_to_remove)  # Aktualizujemy zbiór dostępnych indeksów\n",
    "\n",
    "    return chosen_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 50\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_val_indices(data_length, val_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_train_val_indices(data_length, train_indices, val_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "\n",
    "    # Inicjalizacja figury\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.title(\"Rozkład indeksów walidacyjnych i ich zakresy\")\n",
    "    plt.xlabel(\"Indeksy danych\")\n",
    "    plt.ylabel(\"Wartość (dla wizualizacji)\")\n",
    "\n",
    "    # Rysowanie linii dla całej długości danych\n",
    "    plt.plot([0, data_length - 1], [1, 1], label='Dane', color='blue')\n",
    "\n",
    "    # Rysowanie punktów dla walidacyjnych indeksów\n",
    "    for index in val_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='red', alpha=0.3)  # zakres wokół punktu)\n",
    "    # if train_indices is not empty:\n",
    "    for index in train_indices:\n",
    "        plt.scatter([index], [1], color='red')  # punkt walidacyjny\n",
    "        start = index\n",
    "        end = min(data_length, index + min_distance)\n",
    "        plt.axvspan(start, end, color='blue', alpha=0.1)  # zakres wokół punktu)\n",
    "\n",
    "    plt.legend(['Dane', 'Indeksy walidacyjne i zakres'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 5\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    train_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_val_indices(data_length, val_indices, sequence_length)\n",
    "    plot_train_val_indices(data_length, train_indices, val_indices, sequence_length)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_mask(data_length, chosen_indices, sequence_length):\n",
    "    # Ustal minimalną odległość pomiędzy indeksami\n",
    "    min_distance = sequence_length + 1\n",
    "    # Utwórz maskę początkową ze wszystkimi wartościami ustawionymi na True\n",
    "    mask = np.ones(data_length, dtype=bool)\n",
    "\n",
    "    # Iteruj przez każdy wybrany indeks walidacyjny\n",
    "    for index in chosen_indices:\n",
    "        # Ustal zakres indeksów, które należy ustawić na False\n",
    "        start = max(0, index - min_distance)\n",
    "        end = min(data_length, index + min_distance)\n",
    "\n",
    "        # Ustaw odpowiednie wartości w masce na False\n",
    "        mask[start:end] = False\n",
    "\n",
    "    # Zwróć indeksy, gdzie maska jest True, czyli indeksy zbioru treningowego\n",
    "    training_indices = np.where(mask)[0]  # np.where(mask) zwraca tuple, [0] wyciąga array z indeksami\n",
    "    return training_indices\n",
    "\n",
    "\n",
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 200\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 4\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    plot_validation_indices(data_length, training_indices, val_indices, sequence_length)\n",
    "    print(\"Wygenerowane indeksy walidacyjne:\", val_indices)\n",
    "    print(\"Indeksy zbioru treningowego:\", training_indices)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "randomlist = [1, 2, 9, 10]\n",
    "index = np.random.choice(randomlist)\n",
    "index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Przykładowe wywołanie funkcji\n",
    "data_length = 1000\n",
    "num_of_val_samples = 10\n",
    "sequence_length = 30\n",
    "\n",
    "try:\n",
    "    val_indices = generate_validation_indices(data_length, num_of_val_samples, sequence_length)\n",
    "    training_indices = generate_mask(data_length, val_indices, sequence_length)\n",
    "    print(\"len(training_indices):\", len(training_indices))\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install sidekit\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EEG311)",
   "language": "python",
   "name": "eeg311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
